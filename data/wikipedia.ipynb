{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import glob\n",
    "import pickle\n",
    "import logging\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "from datetime import datetime\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import ftfy\n",
    "import spacy\n",
    "from textacy.datasets.wikipedia import Wikipedia, strip_markup\n",
    "from gensim.models import Word2Vec, FastText\n",
    "from gensim.models.word2vec import PathLineSentences\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "tqdm.monitor_interval = 0\n",
    "\n",
    "logging.basicConfig(filename='log.log', filemode='w', level=logging.INFO, \n",
    "                        format='%(asctime)s %(message)s', datefmt='%m/%d/%Y %I:%M:%S %p')\n",
    "logging.getLogger(\"gensim\").setLevel(logging.WARNING)\n",
    "\n",
    "WIKI_DATA_DIR = '/home/bacon/miniconda3/lib/python3.6/site-packages/textacy/data/wikipedia'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download and prepare Wikipedia data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class WikipediaCorpus:\n",
    "    \n",
    "    MIN_NUM_SENTS_PER_ARTICLE = 10\n",
    "    MIN_NUM_WORDS_IN_SENT = 5\n",
    "    MAX_NUM_WORDS_IN_SENT = 70\n",
    "    \n",
    "    SENT_ENDS = ['.', '!', '?']\n",
    "    TABLE_PREFIX = re.compile(r'\\s*(\\{\\))|(\\|)|(\\|\\})')\n",
    "    TAG = re.compile('<[^<>]+>')\n",
    "    \n",
    "    BAD_POS = ['INTJ', 'SYM', 'X']\n",
    "    \n",
    "    def __init__(self, lg):\n",
    "        self.lg = lg\n",
    "        nlp = spacy.load(lg, disable=['tagger', 'parser', 'ner', 'textcat'])\n",
    "        self.tokenizer = nlp.tokenizer\n",
    "        self.out_dir = os.path.join(lg, 'wikipedia')\n",
    "        os.makedirs(self.out_dir) # errors if out_dir already exists so I don't re-write data\n",
    "    \n",
    "    def download(self):\n",
    "        wp = Wikipedia(lang=self.lg, version='latest', data_dir=WIKI_DATA_DIR)\n",
    "        _ = wp.download()\n",
    "        return\n",
    "    \n",
    "    def prepare(self, N):\n",
    "        self.N = N\n",
    "        self.num_sents = 0\n",
    "        self.num_tokens = 0\n",
    "        self.freq_dist = Counter()\n",
    "        wp = Wikipedia(lang=self.lg, version='latest', data_dir=WIKI_DATA_DIR)\n",
    "        for i, _, content in tqdm(wp):\n",
    "            n, text = self.prepare_one_article(content)\n",
    "            if text:\n",
    "                self.num_sents += n\n",
    "                tokens = text.split()\n",
    "                self.num_tokens += len(tokens)\n",
    "                self.freq_dist.update(tokens)\n",
    "                self.write(text, i)\n",
    "                if self.num_sents >= self.N:\n",
    "                    self.write_metadata()\n",
    "                    return  \n",
    "    \n",
    "    def prepare_one_article(self, content):\n",
    "        content = '\\n'.join([line for line in content.split('\\n') if not self.TABLE_PREFIX.match(line)])\n",
    "        content = strip_markup(content)\n",
    "        lines = self.clean_lines(content)\n",
    "        sentences = []\n",
    "        for line in lines:\n",
    "            for sent in self.tokenize(line):\n",
    "                sentences.append(sent)\n",
    "        article_length = len(sentences)\n",
    "        if article_length >= self.MIN_NUM_SENTS_PER_ARTICLE:\n",
    "            return article_length, '\\n'.join(sentences).lower()\n",
    "        return 0, ''\n",
    "    \n",
    "    def clean_lines(self, content):\n",
    "        content = ftfy.fix_text(content)\n",
    "        for line in content.split('\\n'):\n",
    "            line = self.TAG.sub('', line.strip())\n",
    "            if line and line[0].isalnum():\n",
    "                char_ratio = float(sum(ch.islower() for ch in line)) / sum(not ch.isspace() for ch in line)\n",
    "                if char_ratio > 0.9:\n",
    "                    yield line\n",
    "    \n",
    "    def tokenize(self, content):\n",
    "        tokens = []\n",
    "        for token in self.tokenizer(content):\n",
    "            token = token.text\n",
    "            tokens.append(token)\n",
    "            if token in self.SENT_ENDS:\n",
    "                if self.MIN_NUM_WORDS_IN_SENT <= len(tokens) <= self.MAX_NUM_WORDS_IN_SENT:\n",
    "                    yield ' '.join(tokens)\n",
    "                tokens = []\n",
    "        if tokens:\n",
    "            if self.MIN_NUM_WORDS_IN_SENT <= len(tokens) <= self.MAX_NUM_WORDS_IN_SENT:\n",
    "                yield ' '.join(tokens)\n",
    "                \n",
    "#     def preprocess(self, doc):\n",
    "#         result = []\n",
    "#         for t in doc:\n",
    "#             is_bad_pos = t.pos_ in self.BAD_POS\n",
    "#             is_bad_punct = t.is_punct and (len(t.text) > 1)\n",
    "#             is_bad_token = t.like_url or t.like_email or t.is_space or is_bad_pos or is_bad_punct\n",
    "#             is_num = t.is_digit or t.like_num\n",
    "#             if is_bad_token:\n",
    "#                 break\n",
    "#             elif is_num:\n",
    "#                 text = 'num'\n",
    "#             else:\n",
    "#                 text = t.text.lower()\n",
    "#             result.append(text)\n",
    "#         return result\n",
    "    \n",
    "    def write(self, text, i):\n",
    "        fname = os.path.join(self.out_dir, '{}.txt'.format(i))\n",
    "        with open(fname, encoding='utf-8', mode='w') as f:\n",
    "            f.write(text.replace('\\xa0',' '))\n",
    "    \n",
    "    def write_metadata(self):\n",
    "        metadata = {'num_sents': self.num_sents, 'num_tokens': self.num_tokens,\n",
    "                    'freq_dist': self.freq_dist, 'N': self.N, 'max_length': self.MAX_NUM_WORDS_IN_SENT}\n",
    "        fname = os.path.join(self.lg, 'metadata.pkl')\n",
    "        with open(fname, mode='wb') as f:\n",
    "            pickle.dump(metadata, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clear_dirs(lgs):\n",
    "    for lg in lgs:\n",
    "        cmd = 'rm -rf {}/wikipedia'.format(lg)\n",
    "        os.system(cmd)\n",
    "        cmd = 'rm {}/metadata.pkl'.format(lg)\n",
    "        os.system(cmd)\n",
    "\n",
    "def main(lgs, N):\n",
    "    clear_dirs(lgs)\n",
    "    for lg in lgs:\n",
    "        start = datetime.now()\n",
    "        corpus = WikipediaCorpus(lg)\n",
    "        #corpus.download()\n",
    "        corpus.prepare(N)\n",
    "        end = datetime.now()\n",
    "        msg = 'Processing {} sentences for {} took {}'.format(N, lg, end-start)\n",
    "        logging.info(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0it [00:00, ?it/s]\u001b[A\n",
      "2it [00:00,  4.01it/s]\u001b[A\n",
      "12it [00:00, 15.66it/s]\u001b[A\n",
      "39it [00:00, 44.99it/s]\u001b[A\n",
      "\u001b[A"
     ]
    }
   ],
   "source": [
    "main(['en'], 1000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
