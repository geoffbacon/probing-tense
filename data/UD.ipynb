{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Universal Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import glob\n",
    "import pickle\n",
    "import logging\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=UserWarning)\n",
    "from io import StringIO\n",
    "from datetime import datetime\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "tqdm.monitor_interval = 0\n",
    "import spacy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "logging.basicConfig(filename='logs/UD.log', filemode='w', level=logging.INFO, \n",
    "                        format='%(asctime)s %(message)s', datefmt='%m/%d/%Y %I:%M:%S %p')\n",
    "\n",
    "EN_LINKS = ['https://github.com/UniversalDependencies/UD_English-ParTUT.git',\n",
    "             'https://github.com/UniversalDependencies/UD_English-GUM.git',\n",
    "             'https://github.com/UniversalDependencies/UD_English-EWT.git',\n",
    "             'https://github.com/UniversalDependencies/UD_English-PUD.git',\n",
    "             'https://github.com/UniversalDependencies/UD_English-LinES.git']\n",
    "\n",
    "FR_LINKS = ['https://github.com/UniversalDependencies/UD_French-ParTUT.git',\n",
    "             'https://github.com/UniversalDependencies/UD_French-GSD.git',\n",
    "             'https://github.com/UniversalDependencies/UD_French-Sequoia.git',\n",
    "             'https://github.com/UniversalDependencies/UD_French-PUD.git',\n",
    "             #'https://github.com/UniversalDependencies/UD_French-FTB.git'\n",
    "            ]\n",
    "\n",
    "IT_LINKS = ['https://github.com/UniversalDependencies/UD_Italian-ISDT.git',\n",
    "             'https://github.com/UniversalDependencies/UD_Italian-ParTUT.git',\n",
    "             'https://github.com/UniversalDependencies/UD_Italian-PUD.git']\n",
    "\n",
    "ES_LINKS = ['https://github.com/UniversalDependencies/UD_Spanish-AnCora.git',\n",
    "            'https://github.com/UniversalDependencies/UD_Spanish-GSD.git',\n",
    "            'https://github.com/UniversalDependencies/UD_Spanish-PUD.git']\n",
    "\n",
    "\n",
    "DATASETS = {'en': EN_LINKS,\n",
    "            'fr': FR_LINKS,\n",
    "            'it': IT_LINKS,\n",
    "            'es':  ES_LINKS}\n",
    "\n",
    "UD_DIR = 'UD'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00, 21013.55it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 16777.22it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 13400.33it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 15988.45it/s]\n"
     ]
    }
   ],
   "source": [
    "def download(url, path):\n",
    "    cmd = 'git clone {} {}'.format(url, path)\n",
    "    os.system(cmd)\n",
    "\n",
    "def fetch_datasets(datasets):\n",
    "    for lg, links in datasets.items():\n",
    "        for link in tqdm(links):\n",
    "            dirname = os.path.splitext(os.path.basename(link))[0]\n",
    "            local_path = os.path.join(lg, UD_DIR, dirname)\n",
    "            if not os.path.exists(local_path):\n",
    "                download(link, local_path)\n",
    "\n",
    "fetch_datasets(DATASETS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Sentence:\n",
    "    \n",
    "    TEXT_PATTERN = re.compile(r'#\\s+?text\\s+?=\\s+?(.*)\\n')\n",
    "    WORDLINES_PATTERN = re.compile(r'\\n(1.*\\n)\\n', re.DOTALL)\n",
    "    TENSE_PATTERN = re.compile(r'Tense=([A-Z0-9][a-zA-Z0-9]*)')\n",
    "    VERBFORM_PATTERN = re.compile(r'VerbForm=([A-Z0-9][a-zA-Z0-9]*)')\n",
    "    NULL_VALUE = '_'\n",
    "    COLUMNS = ['id', 'form', 'lemma', 'upos', 'xpos', 'feats', 'head', 'deprel', 'deps', 'misc']\n",
    "    \n",
    "    def __init__(self, raw):\n",
    "        self.text = self.TEXT_PATTERN.search(raw).group(1)\n",
    "        self.words = self.parse_words(raw)\n",
    "        self.root_id = self.words[self.words['head'] == '0'].index[0]\n",
    "        self.tense_id = self.find_tense_id()\n",
    "        self.extract_features()\n",
    "    \n",
    "    def parse_words(self, raw):\n",
    "        words = self.WORDLINES_PATTERN.search(raw).group(1)\n",
    "        words = pd.read_csv(StringIO(words), sep='\\t', names=self.COLUMNS, quoting=3, dtype={'id': str, 'head': str})\n",
    "        is_emptynode = words['id'].str.contains(r'\\.')\n",
    "        words = words[~is_emptynode]\n",
    "        words.set_index('id', inplace=True)\n",
    "        words['tense'] = words['feats'].str.extract(self.TENSE_PATTERN)\n",
    "        words['verbform'] = words['feats'].str.extract(self.VERBFORM_PATTERN)\n",
    "        words.fillna('_', inplace=True)\n",
    "        words['multiword'] = self.NULL_VALUE\n",
    "        is_range = words.index.str.contains('-')\n",
    "        for idx in words.index[is_range]:\n",
    "            start, end = idx.split('-')\n",
    "            ids = map(str, range(int(start), int(end)+1))\n",
    "            for i in ids:\n",
    "                words.loc[i]['multiword'] = idx\n",
    "        return words.drop(['xpos', 'feats', 'deps', 'misc'], axis=1)\n",
    "         \n",
    "    def extract_features(self):\n",
    "        self.length = len(self.words[self.words['multiword'] == self.NULL_VALUE])\n",
    "        if self.tense_id is not None:\n",
    "            self.tense = self.words.loc[self.tense_id]['tense']\n",
    "            self.tense_wordform = self.get_tensed_wordform()\n",
    "            self.tense_lemma = self.words.loc[self.tense_id]['lemma']\n",
    "            self.relation_to_head = self.words.loc[self.tense_id]['deprel']\n",
    "            \n",
    "            all_tensed_words = self.words[self.words['tense'] != self.NULL_VALUE]\n",
    "            self.other_tensed_words = len(all_tensed_words) - 1\n",
    "            self.other_tensed_words_conflicting = (all_tensed_words['tense'] != self.tense).sum()\n",
    "            \n",
    "            next_word = str(int(self.tense_id)+1)\n",
    "            if int(next_word) > int(self.words.index[-1]):\n",
    "                self.dist_from_right = 0\n",
    "                self.other_tensed_words_right = 0\n",
    "                self.other_tensed_words_conflicting_right = 0\n",
    "            else:\n",
    "                words_to_the_right = self.words.loc[next_word:]\n",
    "                self.dist_from_right = (words_to_the_right['multiword'] == self.NULL_VALUE).sum()\n",
    "                tensed_words_to_the_right = words_to_the_right[words_to_the_right['tense'] != self.NULL_VALUE]\n",
    "                self.other_tensed_words_right = len(tensed_words_to_the_right)\n",
    "                self.other_tensed_words_conflicting_right = (tensed_words_to_the_right['tense'] != self.tense).sum()\n",
    "             \n",
    "        else:\n",
    "            self.tense = None\n",
    "            self.tense_wordform = None\n",
    "            self.tense_lemma = None\n",
    "            self.relation_to_head = None\n",
    "            self.dist_from_right = None \n",
    "            self.other_tensed_words = None\n",
    "            self.other_tensed_words_conflicting = None\n",
    "            self.other_tensed_words_right = None\n",
    "            self.other_tensed_words_conflicting_right = None\n",
    "    \n",
    "    def get_tensed_wordform(self):\n",
    "        multiword_id = self.words.loc[self.tense_id]['multiword']\n",
    "        if multiword_id != self.NULL_VALUE:\n",
    "            return self.words.loc[multiword_id]['form']\n",
    "        return self.words.loc[self.tense_id]['form']\n",
    "\n",
    "class EnglishSentence(Sentence):\n",
    "    \n",
    "    def find_tense_id(self):\n",
    "        \"\"\"\n",
    "        1) AUX direct dependent of root, related by (aux, cop or aux:pass), is finite and has tense\n",
    "        2) If found above but no tense, \n",
    "            if lemma is will, take\n",
    "            otherwise UD is wrong\n",
    "        3) VERB that is root, is finite and has tense\n",
    "        \"\"\"\n",
    "        is_aux = self.words['upos'] == 'AUX'\n",
    "        is_dep_head = self.words['head'] == self.root_id\n",
    "        is_aux_cop_deprel = self.words['deprel'].isin(['aux', 'aux:pass', 'cop'])\n",
    "        is_finite = self.words['verbform'] == 'Fin'\n",
    "        has_tense = self.words['tense'] != self.NULL_VALUE\n",
    "        finite_aux = self.words[is_aux & is_dep_head & is_aux_cop_deprel & is_finite]\n",
    "        \n",
    "        if len(finite_aux):\n",
    "            tensed_finite_aux = finite_aux[has_tense]\n",
    "            if len(tensed_finite_aux):\n",
    "                return tensed_finite_aux.index[0]\n",
    "            elif finite_aux['lemma'][0] == 'will':\n",
    "                return finite_aux.index[0]\n",
    "            else:\n",
    "                return None\n",
    "        else:\n",
    "            root = self.words.loc[self.root_id]\n",
    "            if (root['upos'] == 'VERB') and (root['verbform'] == 'Fin') and (root['tense'] != self.NULL_VALUE):\n",
    "                return self.root_id\n",
    "        return None\n",
    "\n",
    "class ItalianSentence(Sentence):\n",
    "    \n",
    "    def find_tensed_word(self):\n",
    "        \"\"\"\n",
    "        1) Root of the sentence if it's a verb and has tense\n",
    "        2) The first direct dependent of the root that is related to the root by cop\n",
    "        3) The first direct dependent of the root that is related to the root by aux\n",
    "        4) The first direct dependent of the root that is an auxiliary\n",
    "        \"\"\"\n",
    "        root = self.words[self.words['head'] == 0].iloc[0]\n",
    "        if (root['upos'] == 'VERB') and (isinstance(root['tense'], str)):\n",
    "            return root\n",
    "        root_id = root['id']\n",
    "        is_dep_of_root = self.words['head'] == root_id\n",
    "        is_copula = self.words['deprel'] == 'cop'\n",
    "        tmp = self.words[is_dep_of_root & is_copula]\n",
    "        if len(tmp) > 0:\n",
    "            return tmp.iloc[0]\n",
    "        is_aux = self.words['deprel'] == 'aux'\n",
    "        tmp = self.words[is_dep_of_root & is_aux]\n",
    "        if len(tmp) > 0:\n",
    "            return tmp.iloc[0]\n",
    "        is_aux = self.words['upos'] == 'AUX'\n",
    "        tmp = self.words[is_dep_of_root & is_aux]\n",
    "        if len(tmp) > 0:\n",
    "            return tmp.iloc[0]\n",
    "        return None\n",
    "\n",
    "class FrenchSentence(Sentence):\n",
    "    \n",
    "    def find_tense_id(self):\n",
    "        \"\"\"\n",
    "        1) Root is verb, is finite and has tense\n",
    "        2) AUX points to head as aux:pass or cop, has tense and is finite\n",
    "        3) AUX points to head, is aux, has tense, is finite\n",
    "            Root is verb, has tense and is part <-\n",
    "            AUX points to head as cop, has tense and is part <-\n",
    "        \n",
    "        4) Root is verb, is non-finite\n",
    "            AUX points to head, is aux, has tense, is finite\n",
    "                if lemma is \"aller\", future\n",
    "                otherwise AUX\n",
    "        \"\"\"\n",
    "        root_is_verb = self.words.loc[self.root_id]['upos'] == 'VERB'\n",
    "        root_is_fin = self.words.loc[self.root_id]['verbform'] == 'Fin'\n",
    "        root_has_tense = self.words.loc[self.root_id]['tense'] != self.NULL_VALUE\n",
    "        if root_is_verb and root_is_fin and root_has_tense:\n",
    "            return self.root_id\n",
    "        is_aux = self.words['upos'] == 'AUX'\n",
    "        is_dep_head = self.words['head'] == self.root_id\n",
    "        is_finite = self.words['verbform'] == 'Fin'\n",
    "        has_tense = self.words['tense'] != self.NULL_VALUE\n",
    "        finite_aux = self.words[is_aux & is_dep_head & is_finite & has_tense]\n",
    "        if len(finite_aux):\n",
    "            is_auxpass_cop_deprel = self.words['deprel'].isin(['aux:pass', 'cop'])\n",
    "            aux_cop_finite_aux = finite_aux[is_auxpass_cop_deprel]\n",
    "            if len(aux_cop_finite_aux):\n",
    "                return aux_cop_finite_aux.index[0]\n",
    "            else:\n",
    "                if root_is_verb and root_has_tense and self.words.loc[self.root_id]['verbform'] == 'Part':\n",
    "                    return self.root_id\n",
    "                else:\n",
    "                    is_cop = self.words['deprel'] == 'cop'\n",
    "                    is_participle = self.words['verbform'] == 'Part'\n",
    "                    aux = self.words[is_aux & is_dep_head & is_cop & has_tense & is_participle]\n",
    "                    if len(aux):\n",
    "                        return aux.index[0]\n",
    "        if root_is_verb and self.words.loc[self.root_id]['verbform'] == 'Inf':\n",
    "            is_aux_deprel = self.words['deprel'] == 'aux'\n",
    "            aux = self.words[is_aux & is_dep_head & is_aux_deprel & has_tense & is_finite]\n",
    "            if len(aux):\n",
    "                return aux.index[0] # doesn't take aller into account\n",
    "        return None\n",
    "\n",
    "class SpanishSentence(Sentence):\n",
    "    \n",
    "    def find_tensed_word(self):\n",
    "        \"\"\"\n",
    "        1) Root of the sentence if it's finite and has tense\n",
    "        2) Root of the sentence if it's a verb and has tense\n",
    "        3) The first direct dependent of the root that is related to the root by cop\n",
    "        4) The first direct dependent of the root that is related to the root by aux\n",
    "        5) The first direct dependent of the root that is an auxiliary\n",
    "        \"\"\"\n",
    "        root = self.words[self.words['head'] == 0].iloc[0]\n",
    "        if (root['verbform'] == 'Fin') and (isinstance(root['tense'], str)):\n",
    "            return root\n",
    "        if (root['upos'] == 'VERB') and (isinstance(root['tense'], str)):\n",
    "            return root\n",
    "        root_id = root['id']\n",
    "        is_dep_of_root = self.words['head'] == root_id\n",
    "        is_copula = self.words['deprel'] == 'cop'\n",
    "        tmp = self.words[is_dep_of_root & is_copula]\n",
    "        if len(tmp) > 0:\n",
    "            return tmp.iloc[0]\n",
    "        is_aux = self.words['deprel'] == 'aux'\n",
    "        tmp = self.words[is_dep_of_root & is_aux]\n",
    "        if len(tmp) > 0:\n",
    "            return tmp.iloc[0]\n",
    "        is_aux = self.words['upos'] == 'AUX'\n",
    "        tmp = self.words[is_dep_of_root & is_aux]\n",
    "        if len(tmp) > 0:\n",
    "            return tmp.iloc[0]\n",
    "        return None\n",
    "\n",
    "class CoNLLUReader:\n",
    "    \n",
    "    SENTENCE_PATTERN = re.compile(r'# sent_id.*?\\n\\n', re.DOTALL)\n",
    "    \n",
    "    SENTENCE_READERS = {'en': EnglishSentence,\n",
    "                        'it': ItalianSentence,\n",
    "                        'fr': FrenchSentence,\n",
    "                        'es': SpanishSentence}\n",
    "    \n",
    "    def __init__(self, lg, fname):\n",
    "        self.lg = lg\n",
    "        self.sentences = self.parse_sentences(self.read(fname))\n",
    "        \n",
    "    def read(self, fname):\n",
    "        with open(fname, encoding='utf-8') as f:\n",
    "            return f.read()\n",
    "        \n",
    "    def parse_sentences(self, raw):\n",
    "        SentenceReader = self.SENTENCE_READERS[self.lg]\n",
    "        sentences = []\n",
    "        for sent in self.SENTENCE_PATTERN.finditer(raw):\n",
    "            sent = SentenceReader(sent.group(0))\n",
    "            sentences.append(sent)\n",
    "        return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class UDReader:\n",
    "    \n",
    "    FEATURES = ['text', 'length', 'tense', 'tense_wordform', 'tense_lemma', 'relation_to_head',\n",
    "                'dist_from_right', 'other_tensed_words', 'other_tensed_words_conflicting',\n",
    "                'other_tensed_words_right', 'other_tensed_words_conflicting_right']\n",
    "    BAD_POS = ['INTJ', 'SYM', 'X']\n",
    "    \n",
    "    def __init__(self, lg):\n",
    "        self.lg = lg\n",
    "        self.nlp = spacy.load(lg)\n",
    "        self.fnames = glob.glob('{}/**/*.conllu'.format(lg))\n",
    "        \n",
    "    def prepare(self):\n",
    "        self.tensed_types = set() # built up during prepare_gt_and_tensed_types()\n",
    "        self.ground_truth = self.prepare_gt_and_tensed_types()\n",
    "        self.freq_dist = self.prepare_freq_dist()\n",
    "        self.responsible_types = self.prepare_responsible()\n",
    "        self.write()\n",
    "    \n",
    "    def prepare_gt_and_tensed_types(self):\n",
    "        gt = []\n",
    "        for fname in tqdm(self.fnames):\n",
    "            reader = CoNLLUReader(self.lg, fname)\n",
    "            for s in reader.sentences:\n",
    "                if s.tense:\n",
    "                    g = self.extract(s, self.FEATURES)\n",
    "                    gt.append(g)\n",
    "                    self.add_tensed_words(s)\n",
    "        return pd.DataFrame(gt, columns=self.FEATURES)\n",
    "        \n",
    "    def prepare_responsible(self):\n",
    "        return set(self.ground_truth['tense_wordform'])\n",
    "    \n",
    "    def add_tensed_words(self, sent):\n",
    "        ids = sent.words[sent.words['tense'] != Sentence.NULL_VALUE].index\n",
    "        for i in ids:\n",
    "            multiword_i = sent.words.loc[i]['multiword']\n",
    "            if multiword_i != Sentence.NULL_VALUE:\n",
    "                i = multiword_i\n",
    "            t = sent.words.loc[i]['form'].lower()\n",
    "            self.tensed_types.add(t)\n",
    "    \n",
    "    def prepare_freq_dist(self):\n",
    "        freq_dist = Counter()\n",
    "        for sent in self.ground_truth['text']:\n",
    "            doc = self.nlp(sent)\n",
    "            tokens = self.preprocess(doc)\n",
    "            freq_dist.update(tokens)\n",
    "        return freq_dist\n",
    "    \n",
    "    def preprocess(self, doc):\n",
    "        result = []\n",
    "        for t in doc:\n",
    "            is_bad_pos = t.pos_ in self.BAD_POS\n",
    "            is_bad_punct = t.is_punct and (len(t.text) > 1)\n",
    "            is_bad_token = t.like_url or t.like_email or t.is_space or is_bad_pos or is_bad_punct\n",
    "            is_num = t.is_digit or t.like_num\n",
    "            if is_bad_token:\n",
    "                break\n",
    "            elif is_num:\n",
    "                text = 'num'\n",
    "            else:\n",
    "                text = t.text.lower()\n",
    "            result.append(text)\n",
    "        return result\n",
    "            \n",
    "    def extract(self, sent, features):\n",
    "        return [getattr(sent, ft) for ft in features]\n",
    "    \n",
    "    def write(self):\n",
    "        ofile = os.path.join(UD_DIR, self.lg, 'ground_truth.csv')\n",
    "        self.ground_truth.to_csv(ofile, index=False)\n",
    "        ofile = os.path.join(UD_DIR, self.lg, 'metadata.pkl')\n",
    "        obj = {'freq_dist': self.freq_dist, 'tensed_types': self.tensed_types, \n",
    "               'responsible_types': self.responsible_types}\n",
    "        with open(ofile, mode='wb') as f:\n",
    "            pickle.dump(obj, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def main(lgs):\n",
    "    for lg in lgs:\n",
    "        start = datetime.now()\n",
    "        reader = UDReader(lg)\n",
    "        reader.prepare()\n",
    "        end = datetime.now()\n",
    "        msg = 'Processing {} took {}'.format(lg, end-start)\n",
    "        logging.info(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#main(['en', 'fr', 'it', 'es'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
