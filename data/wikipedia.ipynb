{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import glob\n",
    "import pickle\n",
    "#import logging\n",
    "from datetime import datetime\n",
    "from collections import Counter\n",
    "import ftfy\n",
    "import spacy\n",
    "from textacy.datasets.wikipedia import Wikipedia, strip_markup\n",
    "from tqdm import tqdm\n",
    "tqdm.monitor_interval = 0\n",
    "\n",
    "#logging.basicConfig(filename='logs/wikipedia.log', filemode='a', level=logging.INFO, \n",
    "#                        format='%(asctime)s %(message)s', datefmt='%m/%d/%Y %I:%M:%S %p')\n",
    "\n",
    "WIKI_DATA_DIR = '/home/bacon/miniconda3/lib/python3.6/site-packages/textacy/data/wikipedia'\n",
    "OUT_DIR = 'wikipedia'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class WikipediaCorpus:\n",
    "    \n",
    "    MIN_NUM_SENTS_PER_ARTICLE = 10\n",
    "    MIN_NUM_WORDS_IN_SENT = 5\n",
    "    MAX_NUM_WORDS_IN_SENT = 70\n",
    "    \n",
    "    SENT_ENDS = ['.', '!', '?']\n",
    "    TABLE_PREFIX = re.compile(r'\\s*(\\{\\))|(\\|)|(\\|\\})')\n",
    "    TAG = re.compile('<[^<>]+>')\n",
    "    \n",
    "    def __init__(self, lg):\n",
    "        self.lg = lg\n",
    "        nlp = spacy.load(lg, disable=['tagger', 'parser', 'ner', 'textcat'])\n",
    "        self.tokenizer = nlp.tokenizer\n",
    "        self.out_dir = os.path.join(OUT_DIR, self.lg, 'articles')\n",
    "        os.makedirs(self.out_dir) # errors if out_dir already exists so I don't re-write data\n",
    "    \n",
    "    def download(self):\n",
    "        wp = Wikipedia(lang=self.lg, version='latest', data_dir=WIKI_DATA_DIR)\n",
    "        _ = wp.download()\n",
    "        return\n",
    "    \n",
    "    def prepare(self, N):\n",
    "        self.N = N\n",
    "        self.num_sents = 0\n",
    "        self.num_tokens = 0\n",
    "        self.freq_dist = Counter()\n",
    "        wp = Wikipedia(lang=self.lg, version='latest', data_dir=WIKI_DATA_DIR)\n",
    "        for i, _, content in tqdm(wp):\n",
    "            n, text = self.prepare_one_article(content)\n",
    "            if text:\n",
    "                self.num_sents += n\n",
    "                tokens = text.split()\n",
    "                self.num_tokens += len(tokens)\n",
    "                self.freq_dist.update(tokens)\n",
    "                self.write(text, i)\n",
    "                if self.num_sents >= self.N:\n",
    "                    self.write_metadata()\n",
    "                    return  \n",
    "    \n",
    "    def prepare_one_article(self, content):\n",
    "        content = '\\n'.join([line for line in content.split('\\n') if not self.TABLE_PREFIX.match(line)])\n",
    "        content = strip_markup(content)\n",
    "        lines = self.clean_lines(content)\n",
    "        sentences = []\n",
    "        for line in lines:\n",
    "            for sent in self.tokenize(line):\n",
    "                sentences.append(sent)\n",
    "        article_length = len(sentences)\n",
    "        if article_length >= self.MIN_NUM_SENTS_PER_ARTICLE:\n",
    "            return article_length, '\\n'.join(sentences).lower()\n",
    "        return 0, ''\n",
    "    \n",
    "    def clean_lines(self, content):\n",
    "        content = ftfy.fix_text(content)\n",
    "        for line in content.split('\\n'):\n",
    "            line = self.TAG.sub('', line.strip())\n",
    "            if line and line[0].isalnum():\n",
    "                char_ratio = float(sum(ch.islower() for ch in line)) / sum(not ch.isspace() for ch in line)\n",
    "                if char_ratio > 0.9:\n",
    "                    yield line\n",
    "    \n",
    "    def tokenize(self, content):\n",
    "        tokens = []\n",
    "        for token in self.tokenizer(content):\n",
    "            token = token.text\n",
    "            tokens.append(token)\n",
    "            if token in self.SENT_ENDS:\n",
    "                if self.MIN_NUM_WORDS_IN_SENT <= len(tokens) <= self.MAX_NUM_WORDS_IN_SENT:\n",
    "                    yield ' '.join(tokens)\n",
    "                tokens = []\n",
    "        if tokens:\n",
    "            if self.MIN_NUM_WORDS_IN_SENT <= len(tokens) <= self.MAX_NUM_WORDS_IN_SENT:\n",
    "                yield ' '.join(tokens)\n",
    "    \n",
    "    def write(self, text, i):\n",
    "        fname = os.path.join(self.out_dir, '{}.txt'.format(i))\n",
    "        with open(fname, encoding='utf-8', mode='w') as f:\n",
    "            text = text.replace('\\xa0',' ')\n",
    "            f.write(text)\n",
    "    \n",
    "    def write_metadata(self):\n",
    "        metadata = {'num_sents': self.num_sents, 'num_tokens': self.num_tokens,\n",
    "                    'freq_dist': self.freq_dist, 'N': self.N}\n",
    "        fname = os.path.join(OUT_DIR, self.lg, 'metadata.pkl')\n",
    "        with open(fname, mode='wb') as f:\n",
    "            pickle.dump(metadata, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def main(lgs, N):\n",
    "    for lg in lgs:\n",
    "        start = datetime.now()\n",
    "        corpus = WikipediaCorpus(lg)\n",
    "        #corpus.download()\n",
    "        corpus.prepare(N)\n",
    "        end = datetime.now()\n",
    "        msg = 'Processing {} sentences for {} took {}'.format(N, lg, end-start)\n",
    "        logging.info(msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Postprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def postprocess(lg, T):\n",
    "    unk_dir = os.path.join(OUT_DIR, lg, 'unk-articles')\n",
    "    os.makedirs(unk_dir) # errors if already exists\n",
    "    UNK = 'UNK'\n",
    "    fname = os.path.join(OUT_DIR, lg, 'metadata.pkl')\n",
    "    with open(fname, 'rb') as f:\n",
    "        obj = pickle.load(f)\n",
    "    wiki_freq_dist = obj['freq_dist']\n",
    "    vocab = set([w for w,f in wiki_freq_dist.most_common(T)])\n",
    "    fname = os.path.join('UD', lg, 'metadata.pkl')\n",
    "    with open(fname, 'rb') as f:\n",
    "        obj = pickle.load(f)\n",
    "    ud_freq_dist = obj['freq_dist']\n",
    "    ud_vocab = set(ud_freq_dist.keys())\n",
    "    not_in_wiki = set()\n",
    "    for t in ud_vocab:\n",
    "        if t in wiki_freq_dist:\n",
    "            vocab.add(t)\n",
    "        else:\n",
    "            not_in_wiki.add(t)\n",
    "    article_fnames = glob.iglob(os.path.join(OUT_DIR, lg, 'articles/*.txt'))\n",
    "    for fname in article_fnames:\n",
    "        with open(fname, encoding='utf-8') as f:\n",
    "            new_lines = []\n",
    "            for line in f:\n",
    "                tokens = line.strip().split(' ')\n",
    "                new_tokens = []\n",
    "                for token in tokens:\n",
    "                    if token in vocab:\n",
    "                        new_tokens.append(token)\n",
    "                    else:\n",
    "                        new_tokens.append(UNK)\n",
    "                new_line = ' '.join(new_tokens)\n",
    "                new_lines.append(new_line)\n",
    "            new_text = '\\n'.join(new_lines)\n",
    "        new_fname = os.path.join(unk_dir, os.path.basename(fname))\n",
    "        with open(new_fname, mode='w', encoding='utf-8') as f:\n",
    "            f.write(new_text)\n",
    "    word2id = {w:i for (w,i) in zip(vocab, range(len(vocab)))}\n",
    "    word2id['UNK'] = len(word2id)\n",
    "    fname = os.path.join(OUT_DIR, lg, 'unk-metadata.pkl')\n",
    "    metadata = {'word2id': word2id, 'not-in-wiki': not_in_wiki}\n",
    "    with open(fname, 'wb') as f:\n",
    "        pickle.dump(metadata, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "N = 1000000 # number of sentences\n",
    "main(['en', 'fr', 'it', 'es'], N)\n",
    "T = 50000 # vocab size\n",
    "for lg in ['en', 'fr', 'it', 'es']:\n",
    "    start = datetime.now()\n",
    "    postprocess(lg, T)\n",
    "    end = datetime.now()\n",
    "    msg = 'Postprocesing {} took {}'.format(lg, end-start)\n",
    "    logging.info(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def check_missing_words(lg):\n",
    "    fname = os.path.join('wikipedia', lg, 'unk-metadata.pkl')\n",
    "    with open(fname, 'rb') as f:\n",
    "        obj = pickle.load(f)\n",
    "    not_in_wiki = obj['not-in-wiki']\n",
    "    fname = os.path.join('UD', lg, 'metadata.pkl')\n",
    "    with open(fname, 'rb') as f:\n",
    "        obj = pickle.load(f)\n",
    "    ud_vocab = obj['freq_dist']\n",
    "    tensed = obj['tensed_types']\n",
    "    responsible = obj['responsible_types']\n",
    "    missing_vocab = Counter({w:f for w,f in ud_vocab.items() if w in not_in_wiki})\n",
    "    return (tensed & not_in_wiki), (responsible & not_in_wiki), missing_vocab\n",
    "\n",
    "def check_unk_proportion(lg):\n",
    "    fname = os.path.join('wikipedia', lg, 'metadata.pkl')\n",
    "    with open(fname, 'rb') as f:\n",
    "        obj = pickle.load(f)\n",
    "    wiki_freq_dist = obj['freq_dist']\n",
    "    num_tokens = obj['num_tokens']\n",
    "    fname = os.path.join('wikipedia', lg, 'unk-metadata.pkl')\n",
    "    with open(fname, 'rb') as f:\n",
    "        obj = pickle.load(f)\n",
    "    word2id = obj['word2id']\n",
    "    num_unks = 0\n",
    "    for word in wiki_freq_dist:\n",
    "        if word not in word2id:\n",
    "            num_unks += wiki_freq_dist[word]\n",
    "    return num_unks / num_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "en\n",
      "Missing vocab:  2692\n",
      "Missing tense:  133\n",
      "Missing responsible:  38\n",
      "Number of UNKs:  0.02040813786045971\n",
      "\n",
      "fr\n",
      "Missing vocab:  9455\n",
      "Missing tense:  679\n",
      "Missing responsible:  294\n",
      "Number of UNKs:  0.023629998821879106\n",
      "\n",
      "it\n",
      "Missing vocab:  1894\n",
      "Missing tense:  319\n",
      "Missing responsible:  164\n",
      "Number of UNKs:  0.04170310933736561\n",
      "\n",
      "es\n",
      "Missing vocab:  9561\n",
      "Missing tense:  842\n",
      "Missing responsible:  300\n",
      "Number of UNKs:  0.02912028187030663\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for lg in ['en', 'fr', 'it', 'es']:\n",
    "    missing_tense, missing_responsible, missing_vocab = check_missing_words(lg)\n",
    "    print(lg)\n",
    "    print('Missing vocab: ', len(missing_vocab))\n",
    "    print('Missing tense: ', len(missing_tense))\n",
    "    print('Missing responsible: ', len(missing_responsible))\n",
    "    print('Number of UNKs: ', check_unk_proportion(lg))\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
