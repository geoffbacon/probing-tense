{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import glob\n",
    "import pickle\n",
    "import logging\n",
    "from datetime import datetime\n",
    "from collections import Counter\n",
    "import ftfy\n",
    "import spacy\n",
    "from textacy.datasets.wikipedia import Wikipedia, strip_markup\n",
    "from tqdm import tqdm\n",
    "tqdm.monitor_interval = 0\n",
    "\n",
    "logging.basicConfig(filename='logs/wikipedia.log', filemode='a', level=logging.INFO, \n",
    "                        format='%(asctime)s %(message)s', datefmt='%m/%d/%Y %I:%M:%S %p')\n",
    "\n",
    "WIKI_DATA_DIR = '/home/bacon/miniconda3/lib/python3.6/site-packages/textacy/data/wikipedia'\n",
    "OUT_DIR = 'wikipedia'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class WikipediaCorpus:\n",
    "    \n",
    "    MIN_NUM_SENTS_PER_ARTICLE = 10\n",
    "    MIN_NUM_WORDS_IN_SENT = 5\n",
    "    MAX_NUM_WORDS_IN_SENT = 70\n",
    "    \n",
    "    SENT_ENDS = ['.', '!', '?']\n",
    "    TABLE_PREFIX = re.compile(r'\\s*(\\{\\))|(\\|)|(\\|\\})')\n",
    "    TAG = re.compile('<[^<>]+>')\n",
    "    \n",
    "    def __init__(self, lg):\n",
    "        self.lg = lg\n",
    "        nlp = spacy.load(lg, disable=['tagger', 'parser', 'ner', 'textcat'])\n",
    "        self.tokenizer = nlp.tokenizer\n",
    "        self.out_dir = os.path.join(OUT_DIR, self.lg, 'articles')\n",
    "        os.makedirs(self.out_dir) # errors if out_dir already exists so I don't re-write data\n",
    "    \n",
    "    def download(self):\n",
    "        wp = Wikipedia(lang=self.lg, version='latest', data_dir=WIKI_DATA_DIR)\n",
    "        _ = wp.download()\n",
    "        return\n",
    "    \n",
    "    def prepare(self, N):\n",
    "        self.N = N\n",
    "        self.num_sents = 0\n",
    "        self.num_tokens = 0\n",
    "        self.freq_dist = Counter()\n",
    "        wp = Wikipedia(lang=self.lg, version='latest', data_dir=WIKI_DATA_DIR)\n",
    "        for i, _, content in tqdm(wp):\n",
    "            n, text = self.prepare_one_article(content)\n",
    "            if text:\n",
    "                self.num_sents += n\n",
    "                tokens = text.split()\n",
    "                self.num_tokens += len(tokens)\n",
    "                self.freq_dist.update(tokens)\n",
    "                self.write(text, i)\n",
    "                if self.num_sents >= self.N:\n",
    "                    self.write_metadata()\n",
    "                    return  \n",
    "    \n",
    "    def prepare_one_article(self, content):\n",
    "        content = '\\n'.join([line for line in content.split('\\n') if not self.TABLE_PREFIX.match(line)])\n",
    "        content = strip_markup(content)\n",
    "        lines = self.clean_lines(content)\n",
    "        sentences = []\n",
    "        for line in lines:\n",
    "            for sent in self.tokenize(line):\n",
    "                sentences.append(sent)\n",
    "        article_length = len(sentences)\n",
    "        if article_length >= self.MIN_NUM_SENTS_PER_ARTICLE:\n",
    "            return article_length, '\\n'.join(sentences).lower()\n",
    "        return 0, ''\n",
    "    \n",
    "    def clean_lines(self, content):\n",
    "        content = ftfy.fix_text(content)\n",
    "        for line in content.split('\\n'):\n",
    "            line = self.TAG.sub('', line.strip())\n",
    "            if line and line[0].isalnum():\n",
    "                char_ratio = float(sum(ch.islower() for ch in line)) / sum(not ch.isspace() for ch in line)\n",
    "                if char_ratio > 0.9:\n",
    "                    yield line\n",
    "    \n",
    "    def tokenize(self, content):\n",
    "        tokens = []\n",
    "        for token in self.tokenizer(content):\n",
    "            token = token.text\n",
    "            tokens.append(token)\n",
    "            if token in self.SENT_ENDS:\n",
    "                if self.MIN_NUM_WORDS_IN_SENT <= len(tokens) <= self.MAX_NUM_WORDS_IN_SENT:\n",
    "                    yield ' '.join(tokens)\n",
    "                tokens = []\n",
    "        if tokens:\n",
    "            if self.MIN_NUM_WORDS_IN_SENT <= len(tokens) <= self.MAX_NUM_WORDS_IN_SENT:\n",
    "                yield ' '.join(tokens)\n",
    "    \n",
    "    def write(self, text, i):\n",
    "        fname = os.path.join(self.out_dir, '{}.txt'.format(i))\n",
    "        with open(fname, encoding='utf-8', mode='w') as f:\n",
    "            text = text.replace('\\xa0',' ')\n",
    "            f.write(text)\n",
    "    \n",
    "    def write_metadata(self):\n",
    "        metadata = {'num_sents': self.num_sents, 'num_tokens': self.num_tokens,\n",
    "                    'freq_dist': self.freq_dist, 'N': self.N}\n",
    "        fname = os.path.join(OUT_DIR, self.lg, 'metadata.pkl')\n",
    "        with open(fname, mode='wb') as f:\n",
    "            pickle.dump(metadata, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def main(lgs, N):\n",
    "    for lg in lgs:\n",
    "        start = datetime.now()\n",
    "        corpus = WikipediaCorpus(lg)\n",
    "        #corpus.download()\n",
    "        corpus.prepare(N)\n",
    "        end = datetime.now()\n",
    "        msg = 'Processing {} sentences for {} took {}'.format(N, lg, end-start)\n",
    "        logging.info(msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Postprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def postprocess(lg, T):\n",
    "    unk_dir = os.path.join(OUT_DIR, lg, 'unk-articles')\n",
    "    os.makedirs(unk_dir) # errors if already exists\n",
    "    UNK = 'UNK'\n",
    "    fname = os.path.join(OUT_DIR, lg, 'metadata.pkl')\n",
    "    with open(fname, 'rb') as f:\n",
    "        obj = pickle.load(f)\n",
    "    wiki_freq_dist = obj['freq_dist']\n",
    "    vocab = set([w for w,f in wiki_freq_dist.most_common(T)])\n",
    "    fname = os.path.join('UD', lg, 'metadata.pkl')\n",
    "    with open(fname, 'rb') as f:\n",
    "        obj = pickle.load(f)\n",
    "    ud_freq_dist = obj['freq_dist']\n",
    "    ud_vocab = set(ud_freq_dist.keys())\n",
    "    not_in_wiki = set()\n",
    "    for t in ud_vocab:\n",
    "        if t in wiki_freq_dist:\n",
    "            vocab.add(t)\n",
    "        else:\n",
    "            not_in_wiki.add(t)\n",
    "    article_fnames = glob.iglob(os.path.join(OUT_DIR, lg, 'articles/*.txt'))\n",
    "    for fname in article_fnames:\n",
    "        with open(fname, encoding='utf-8') as f:\n",
    "            new_lines = []\n",
    "            for line in f:\n",
    "                tokens = line.strip().split(' ')\n",
    "                new_tokens = []\n",
    "                for token in tokens:\n",
    "                    if token in vocab:\n",
    "                        new_tokens.append(token)\n",
    "                    else:\n",
    "                        new_tokens.append(UNK)\n",
    "                new_line = ' '.join(new_tokens)\n",
    "                new_lines.append(new_line)\n",
    "            new_text = '\\n'.join(new_lines)\n",
    "        new_fname = os.path.join(unk_dir, os.path.basename(fname))\n",
    "        with open(new_fname, mode='w', encoding='utf-8') as f:\n",
    "            f.write(new_text)\n",
    "    word2id = {w:i for (w,i) in zip(vocab, range(1, len(vocab)+1))}\n",
    "    word2id['UNK'] = len(word2id) + 1\n",
    "    fname = os.path.join(OUT_DIR, lg, 'unk-metadata.pkl')\n",
    "    metadata = {'word2id': word2id, 'not-in-wiki': not_in_wiki}\n",
    "    with open(fname, 'wb') as f:\n",
    "        pickle.dump(metadata, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [17:26<00:00, 261.63s/it]\n"
     ]
    }
   ],
   "source": [
    "# N = 1000000 # number of sentences\n",
    "# main(['en', 'fr', 'it', 'es'], N)\n",
    "# T = 50000 # vocab size\n",
    "# for lg in tqdm(['en', 'fr', 'it', 'es']):\n",
    "#     start = datetime.now()\n",
    "#     postprocess(lg, T)\n",
    "#     end = datetime.now()\n",
    "#     msg = 'Postprocesing {} took {}'.format(lg, end-start)\n",
    "#     logging.info(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def check_missing_words(lg):\n",
    "    fname = os.path.join('wikipedia', lg, 'unk-metadata.pkl')\n",
    "    with open(fname, 'rb') as f:\n",
    "        obj = pickle.load(f)\n",
    "    not_in_wiki = obj['not-in-wiki']\n",
    "    fname = os.path.join('UD', lg, 'metadata.pkl')\n",
    "    with open(fname, 'rb') as f:\n",
    "        obj = pickle.load(f)\n",
    "    ud_vocab = obj['freq_dist']\n",
    "    tensed = obj['tensed_types']\n",
    "    responsible = obj['responsible_types']\n",
    "    missing_vocab = Counter({w:f for w,f in ud_vocab.items() if w in not_in_wiki})\n",
    "    return (tensed & not_in_wiki), (responsible & not_in_wiki), missing_vocab\n",
    "\n",
    "def check_unk_proportion(lg):\n",
    "    fname = os.path.join('wikipedia', lg, 'metadata.pkl')\n",
    "    with open(fname, 'rb') as f:\n",
    "        obj = pickle.load(f)\n",
    "    wiki_freq_dist = obj['freq_dist']\n",
    "    num_tokens = obj['num_tokens']\n",
    "    fname = os.path.join('wikipedia', lg, 'unk-metadata.pkl')\n",
    "    with open(fname, 'rb') as f:\n",
    "        obj = pickle.load(f)\n",
    "    word2id = obj['word2id']\n",
    "    num_unks = 0\n",
    "    for word in wiki_freq_dist:\n",
    "        if word not in word2id:\n",
    "            num_unks += wiki_freq_dist[word]\n",
    "    return num_unks / num_tokens\n",
    "\n",
    "# for lg in ['en', 'fr', 'it', 'es']:\n",
    "#     missing_tense, missing_responsible, missing_vocab = check_missing_words(lg)\n",
    "#     print(lg)\n",
    "#     print('Missing vocab: ', len(missing_vocab))\n",
    "#     print('Missing tense: ', len(missing_tense))\n",
    "#     print('Missing responsible: ', len(missing_responsible))\n",
    "#     print('Number of UNKs: ', check_unk_proportion(lg))\n",
    "#     print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Postpostprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "def indices_from_sentence(sent, w2i):\n",
    "        return [w2i['SOS']] + [w2i[w] for w in sent.split(' ') if w] + [w2i['EOS'], w2i['PAD']]\n",
    "\n",
    "def make_batched_data(lg, B):\n",
    "    fname = os.path.join(OUT_DIR, lg, 'unk-metadata.pkl')\n",
    "    with open(fname, 'rb') as f:\n",
    "        obj = pickle.load(f)\n",
    "    word2id = obj['word2id']\n",
    "    word2id['PAD'] = 0\n",
    "    word2id['SOS'] = len(word2id)\n",
    "    word2id['EOS'] = len(word2id)\n",
    "    vocab_size = len(word2id)\n",
    "    fname = os.path.join(OUT_DIR, lg, 'batched-metadata.pkl')\n",
    "    obj = {'word2id': word2id, 'vocab-size': vocab_size, 'batch_size': B}\n",
    "    with open(fname, 'wb') as f:\n",
    "        pickle.dump(obj, f)\n",
    "\n",
    "    fnames = glob.iglob(os.path.join(OUT_DIR, lg, 'unk-articles/*.txt'))\n",
    "    dirname = os.path.join(OUT_DIR, lg, 'batched-articles')\n",
    "    os.makedirs(dirname)\n",
    "    batch = []\n",
    "    i = 0\n",
    "    for fname in tqdm(fnames):\n",
    "        with open(fname, encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                idx = indices_from_sentence(line, word2id)\n",
    "                batch.append(idx)\n",
    "                if len(batch) == B:\n",
    "                    data = pad_sequences(batch, padding='post', value=word2id['PAD'])\n",
    "                    df = pd.DataFrame(data)\n",
    "                    ofile = os.path.join(dirname, '{}.csv'.format(i))\n",
    "                    df.to_csv(ofile, header=False, index=False, encoding='utf-8')\n",
    "                    i += 1\n",
    "                    batch = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10960it [01:38, 111.31it/s]\n"
     ]
    }
   ],
   "source": [
    "#make_batched_data('en', 32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Counting lemma frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def count_lemmata(lg):\n",
    "    start = datetime.now()\n",
    "    nlp = spacy.load(lg)\n",
    "    fnames = glob.iglob(os.path.join('wikipedia', lg, 'unk-articles/*.txt'))\n",
    "    lemmata = Counter()\n",
    "    for fname in fnames:\n",
    "        with open(fname, encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                doc = nlp(line)\n",
    "                lemmata.update([token.lemma_ for token in doc])\n",
    "    fname = os.path.join('wikipedia', lg, 'lemmata.pkl')\n",
    "    with open(fname, 'wb') as f:\n",
    "        pickle.dump(lemmata, f)\n",
    "    end = datetime.now()\n",
    "    msg = 'Counting lemmata for {} took {}'.format(lg, end-start)\n",
    "    logging.info(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-7154ac93bf5e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'fr'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'es'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'it'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mcount_lemmata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-4-016724d01a0d>\u001b[0m in \u001b[0;36mcount_lemmata\u001b[0;34m(lg)\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m                 \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m                 \u001b[0mlemmata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlemma_\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mfname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'wikipedia'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'lemmata.pkl'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/spacy/language.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, text, disable)\u001b[0m\n\u001b[1;32m    350\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__call__'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE003\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomponent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 352\u001b[0;31m             \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mproc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    353\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE005\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mnn_parser.pyx\u001b[0m in \u001b[0;36mspacy.syntax.nn_parser.Parser.__call__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mnn_parser.pyx\u001b[0m in \u001b[0;36mspacy.syntax.nn_parser.Parser.parse_batch\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for lg in ['fr', 'es', 'it']:\n",
    "    count_lemmata(lg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
