{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pickle\n",
    "import logging\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "tqdm.monitor_interval = 0\n",
    "import pandas as pd\n",
    "\n",
    "logging.basicConfig(filename='logs/autoencoder.log', filemode='a', level=logging.INFO, \n",
    "                        format='%(asctime)s %(message)s', datefmt='%m/%d/%Y %I:%M:%S %p')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "from masked_cross_entropy import *\n",
    "\n",
    "device = torch.device('cpu')\n",
    "\n",
    "OUT_DIR = 'embeddings'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dim = 8\n",
    "lg = 'en'\n",
    "fnames = glob.iglob(os.path.join('wikipedia', lg, 'unk-articles/1111*.txt'))\n",
    "fname = os.path.join('wikipedia', lg, 'unk-metadata.pkl')\n",
    "with open(fname, 'rb') as f:\n",
    "    obj = pickle.load(f)\n",
    "word2id = obj['word2id']\n",
    "word2id['SOS'] = len(word2id)\n",
    "word2id['EOS'] = len(word2id)\n",
    "word2id['PAD'] = len(word2id)\n",
    "def read():\n",
    "    for fname in fnames:\n",
    "        with open(fname, encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                yield line.strip()\n",
    "sentences = list(read())\n",
    "SOS = word2id['SOS']\n",
    "EOS = word2id['EOS']\n",
    "PAD = word2id['PAD']\n",
    "vocab_size = len(word2id)\n",
    "\n",
    "def indices_from_sentence(sentence):\n",
    "    return [word2id[word] for word in sentence.split(' ') if word]\n",
    "# Above is all the Trainer stuff\n",
    "\n",
    "def pad_seq(seq, max_length):\n",
    "    seq += [PAD for i in range(max_length - len(seq))]\n",
    "    return seq\n",
    "\n",
    "# prepare batch\n",
    "BATCH_SIZE = 3\n",
    "batch = []\n",
    "for i in range(BATCH_SIZE):\n",
    "    tokens = indices_from_sentence(sentences[i])\n",
    "    batch.append(tokens)\n",
    "batch.sort(key=lambda s: len(s), reverse=True)\n",
    "batch_lengths = [len(s) for s in batch]\n",
    "max_length = max(batch_lengths)\n",
    "padded_batch = [pad_seq(s, max_length) for s in batch]\n",
    "input_var = Variable(torch.LongTensor(padded_batch)).transpose(0, 1) # just a tensor\n",
    "# end prepare batch\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.lstm = nn.LSTM(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, input_seq, input_lengths, hidden=None):\n",
    "        embedded = self.embedding(input_seq)\n",
    "        packed = pack_padded_sequence(embedded, input_lengths)\n",
    "        output_seq, hidden = self.lstm(packed, hidden)\n",
    "        output_seq, output_lengths = pad_packed_sequence(output_seq)\n",
    "        return output_seq, hidden\n",
    "\n",
    "    def init_hidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, batch_size):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.batch_size=batch_size\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.lstm = nn.LSTM(hidden_size, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "    \n",
    "    def forward(self, input_seq, last_hidden):\n",
    "        embedded = self.embedding(input_seq).view(1, self.batch_size, self.hidden_size) # S=1 x B x N\n",
    "        rnn_output, hidden = self.lstm(embedded, last_hidden)\n",
    "        output = self.softmax(self.out(rnn_output))\n",
    "        return output, hidden\n",
    "    \n",
    "    def init_hidden(self):\n",
    "        return torch.zeros(1, self.batch_size, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr=0.01\n",
    "encoder = Encoder(vocab_size, dim)\n",
    "decoder = Decoder(dim, vocab_size, BATCH_SIZE)\n",
    "encoder_outputs, (encoder_hidden, encoder_cell) = encoder(input_var, batch_lengths)\n",
    "encoder_optimizer = optim.SGD(encoder.parameters(), lr=lr)\n",
    "decoder_optimizer = optim.SGD(decoder.parameters(), lr=lr)\n",
    "encoder_optimizer.zero_grad()\n",
    "decoder_optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "decoder_input = Variable(torch.LongTensor([SOS] * BATCH_SIZE))\n",
    "decoder_hidden = encoder_hidden[:1]\n",
    "all_decoder_outputs = Variable(torch.zeros(max_length, BATCH_SIZE, vocab_size))\n",
    "decoder_cell = decoder.init_hidden()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for t in range(max_length):\n",
    "    decoder_output, (decoder_hidden, decoder_cell) = decoder(decoder_input, (decoder_hidden, decoder_cell))\n",
    "    all_decoder_outputs[t] = decoder_output\n",
    "    decoder_input = input_var[t]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss = masked_cross_entropy(\n",
    "    all_decoder_outputs.transpose(0, 1).contiguous(),\n",
    "    input_var.transpose(0, 1).contiguous(),\n",
    "    batch_lengths\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
