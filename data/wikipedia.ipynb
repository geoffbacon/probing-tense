{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import glob\n",
    "import pickle\n",
    "import logging\n",
    "from datetime import datetime\n",
    "from collections import Counter\n",
    "import ftfy\n",
    "import spacy\n",
    "from textacy.datasets.wikipedia import Wikipedia, strip_markup\n",
    "from tqdm import tqdm\n",
    "tqdm.monitor_interval = 0\n",
    "\n",
    "logging.basicConfig(filename='logs/wikipedia.log', filemode='w', level=logging.INFO, \n",
    "                        format='%(asctime)s %(message)s', datefmt='%m/%d/%Y %I:%M:%S %p')\n",
    "\n",
    "WIKI_DATA_DIR = '/home/bacon/miniconda3/lib/python3.6/site-packages/textacy/data/wikipedia'\n",
    "OUT_DIR = 'wikipedia'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class WikipediaCorpus:\n",
    "    \n",
    "    MIN_NUM_SENTS_PER_ARTICLE = 10\n",
    "    MIN_NUM_WORDS_IN_SENT = 5\n",
    "    MAX_NUM_WORDS_IN_SENT = 70\n",
    "    \n",
    "    SENT_ENDS = ['.', '!', '?']\n",
    "    TABLE_PREFIX = re.compile(r'\\s*(\\{\\))|(\\|)|(\\|\\})')\n",
    "    TAG = re.compile('<[^<>]+>')\n",
    "    \n",
    "    BAD_POS = ['INTJ', 'SYM', 'X']\n",
    "    \n",
    "    def __init__(self, lg):\n",
    "        self.lg = lg\n",
    "        nlp = spacy.load(lg, disable=['tagger', 'parser', 'ner', 'textcat'])\n",
    "        self.tokenizer = nlp.tokenizer\n",
    "        self.out_dir = os.path.join(OUT_DIR, self.lg, 'articles')\n",
    "        os.makedirs(self.out_dir) # errors if out_dir already exists so I don't re-write data\n",
    "    \n",
    "    def download(self):\n",
    "        wp = Wikipedia(lang=self.lg, version='latest', data_dir=WIKI_DATA_DIR)\n",
    "        _ = wp.download()\n",
    "        return\n",
    "    \n",
    "    def prepare(self, N):\n",
    "        self.N = N\n",
    "        self.num_sents = 0\n",
    "        self.num_tokens = 0\n",
    "        self.freq_dist = Counter()\n",
    "        wp = Wikipedia(lang=self.lg, version='latest', data_dir=WIKI_DATA_DIR)\n",
    "        for i, _, content in tqdm(wp):\n",
    "            n, text = self.prepare_one_article(content)\n",
    "            if text:\n",
    "                self.num_sents += n\n",
    "                tokens = text.split()\n",
    "                self.num_tokens += len(tokens)\n",
    "                self.freq_dist.update(tokens)\n",
    "                self.write(text, i)\n",
    "                if self.num_sents >= self.N:\n",
    "                    self.write_metadata()\n",
    "                    return  \n",
    "    \n",
    "    def prepare_one_article(self, content):\n",
    "        content = '\\n'.join([line for line in content.split('\\n') if not self.TABLE_PREFIX.match(line)])\n",
    "        content = strip_markup(content)\n",
    "        lines = self.clean_lines(content)\n",
    "        sentences = []\n",
    "        for line in lines:\n",
    "            for sent in self.tokenize(line):\n",
    "                sentences.append(sent)\n",
    "        article_length = len(sentences)\n",
    "        if article_length >= self.MIN_NUM_SENTS_PER_ARTICLE:\n",
    "            return article_length, '\\n'.join(sentences).lower()\n",
    "        return 0, ''\n",
    "    \n",
    "    def clean_lines(self, content):\n",
    "        content = ftfy.fix_text(content)\n",
    "        for line in content.split('\\n'):\n",
    "            line = self.TAG.sub('', line.strip())\n",
    "            if line and line[0].isalnum():\n",
    "                char_ratio = float(sum(ch.islower() for ch in line)) / sum(not ch.isspace() for ch in line)\n",
    "                if char_ratio > 0.9:\n",
    "                    yield line\n",
    "    \n",
    "    def tokenize(self, content):\n",
    "        tokens = []\n",
    "        for token in self.tokenizer(content):\n",
    "            token = token.text\n",
    "            tokens.append(token)\n",
    "            if token in self.SENT_ENDS:\n",
    "                if self.MIN_NUM_WORDS_IN_SENT <= len(tokens) <= self.MAX_NUM_WORDS_IN_SENT:\n",
    "                    yield ' '.join(tokens)\n",
    "                tokens = []\n",
    "        if tokens:\n",
    "            if self.MIN_NUM_WORDS_IN_SENT <= len(tokens) <= self.MAX_NUM_WORDS_IN_SENT:\n",
    "                yield ' '.join(tokens)\n",
    "    \n",
    "    def write(self, text, i):\n",
    "        fname = os.path.join(self.out_dir, '{}.txt'.format(i))\n",
    "        with open(fname, encoding='utf-8', mode='w') as f:\n",
    "            text = text.replace('\\xa0',' ')\n",
    "            f.write(text)\n",
    "    \n",
    "    def write_metadata(self):\n",
    "        metadata = {'num_sents': self.num_sents, 'num_tokens': self.num_tokens,\n",
    "                    'freq_dist': self.freq_dist, 'N': self.N}\n",
    "        fname = os.path.join(OUT_DIR, self.lg, 'metadata.pkl')\n",
    "        with open(fname, mode='wb') as f:\n",
    "            pickle.dump(metadata, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def main(lgs, N):\n",
    "    for lg in lgs:\n",
    "        start = datetime.now()\n",
    "        corpus = WikipediaCorpus(lg)\n",
    "        #corpus.download()\n",
    "        corpus.prepare(N)\n",
    "        end = datetime.now()\n",
    "        msg = 'Processing {} sentences for {} took {}'.format(N, lg, end-start)\n",
    "        logging.info(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "47it [00:00, 48.81it/s]\n"
     ]
    }
   ],
   "source": [
    "main(['en', 'fr', 'it', 'es'], 1000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
